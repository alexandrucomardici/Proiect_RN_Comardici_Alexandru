{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663c213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0b37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ANNOTATIONS = Path(r\"A:\\cats_dogs_light\\Proiect RN COMARDICI ALEXANDRU\\dataset\\annotations.csv\")\n",
    "IMAGES_DIR = Path(r\"A:\\cats_dogs_light\\Proiect RN COMARDICI ALEXANDRU\\dataset\\train\")\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "MODEL_SAVE_PATH = Path(r\"A:\\cats_dogs_light\\Proiect RN COMARDICI ALEXANDRU\\dataset\\animal_detector.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a925d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(ANNOTATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ead952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode columns\n",
    "df['species_enc'] = df['species'].map({'cat':0, 'dog':1})\n",
    "size_classes = ['short','medium','long']\n",
    "df['size_enc'] = df['size'].apply(lambda s: [1 if s==c else 0 for c in size_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b276aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, stratify=df['species_enc'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b15c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to load images + labels\n",
    "def df_to_numpy(df, img_dir):\n",
    "    X, y_species, y_owner, y_size = [], [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = img_dir / row['filename']\n",
    "        img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "        arr = image.img_to_array(img)/255.0\n",
    "        X.append(arr)\n",
    "        y_species.append(to_categorical(row['species_enc'], 2))\n",
    "        y_owner.append(np.array([row['has_owner']], dtype=np.float32))\n",
    "        y_size.append(np.array(row['size_enc'], dtype=np.float32))\n",
    "    return np.array(X), {'species': np.array(y_species), \n",
    "                         'has_owner': np.array(y_owner), \n",
    "                         'size': np.array(y_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f98d65",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, y_train_dict = \u001b[43mdf_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGES_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m X_val, y_val_dict = df_to_numpy(val_df, IMAGES_DIR)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mdf_to_numpy\u001b[39m\u001b[34m(df, img_dir)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows():\n\u001b[32m      5\u001b[39m     img_path = img_dir / row[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     img = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     arr = image.img_to_array(img)/\u001b[32m255.0\u001b[39m\n\u001b[32m      8\u001b[39m     X.append(arr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\utils\\image_utils.py:235\u001b[39m, in \u001b[36mload_img\u001b[39m\u001b[34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[39m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib.Path):\n\u001b[32m    234\u001b[39m         path = \u001b[38;5;28mstr\u001b[39m(path.resolve())\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    236\u001b[39m         img = pil_image.open(io.BytesIO(f.read()))\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train_dict = df_to_numpy(train_df, IMAGES_DIR)\n",
    "X_val, y_val_dict = df_to_numpy(val_df, IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba24ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base = MobileNetV2(input_shape=IMG_SIZE+(3,), include_top=False, weights='imagenet')\n",
    "base.trainable = False\n",
    "\n",
    "inputs = layers.Input(shape=IMG_SIZE+(3,))\n",
    "x = base(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "\n",
    "species_out = layers.Dense(2, activation='softmax', name='species')(x)\n",
    "owner_out = layers.Dense(1, activation='sigmoid', name='has_owner')(x)\n",
    "size_out = layers.Dense(3, activation='softmax', name='size')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=[species_out, owner_out, size_out])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'species':'categorical_crossentropy',\n",
    "          'has_owner':'binary_crossentropy',\n",
    "          'size':'categorical_crossentropy'},\n",
    "    metrics={'species':'accuracy','has_owner':'accuracy','size':'accuracy'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e93e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight_train = {\n",
    "    'species': np.ones(len(train_df), dtype=np.float32),\n",
    "    'has_owner': np.ones(len(train_df), dtype=np.float32),\n",
    "    'size': np.ones(len(train_df), dtype=np.float32)\n",
    "}\n",
    "\n",
    "sample_weight_val = {\n",
    "    'species': np.ones(len(val_df), dtype=np.float32),\n",
    "    'has_owner': np.ones(len(val_df), dtype=np.float32),\n",
    "    'size': np.ones(len(val_df), dtype=np.float32)\n",
    "}\n",
    "\n",
    "# Callbacks\n",
    "cb = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        save_best_only=True,\n",
    "        monitor='val_species_accuracy',\n",
    "        mode='max'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c59405fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 228ms/step - has_owner_accuracy: 0.7693 - has_owner_loss: 0.4579 - loss: 1.2247 - size_accuracy: 0.7553 - size_loss: 0.6092 - species_accuracy: 0.9379 - species_loss: 0.1561 - val_has_owner_accuracy: 0.7417 - val_has_owner_loss: 0.4603 - val_loss: 1.1619 - val_size_accuracy: 0.7417 - val_size_loss: 0.6458 - val_species_accuracy: 0.9801 - val_species_loss: 0.0726\n",
      "Epoch 2/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 203ms/step - has_owner_accuracy: 0.8560 - has_owner_loss: 0.3285 - loss: 0.7686 - size_accuracy: 0.8302 - size_loss: 0.3893 - species_accuracy: 0.9836 - species_loss: 0.0426 - val_has_owner_accuracy: 0.7748 - val_has_owner_loss: 0.4219 - val_loss: 0.9986 - val_size_accuracy: 0.7881 - val_size_loss: 0.4978 - val_species_accuracy: 0.9669 - val_species_loss: 0.0802\n",
      "Epoch 3/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 197ms/step - has_owner_accuracy: 0.8759 - has_owner_loss: 0.2772 - loss: 0.6092 - size_accuracy: 0.8677 - size_loss: 0.3104 - species_accuracy: 0.9941 - species_loss: 0.0227 - val_has_owner_accuracy: 0.7682 - val_has_owner_loss: 0.4378 - val_loss: 1.0911 - val_size_accuracy: 0.7483 - val_size_loss: 0.5791 - val_species_accuracy: 0.9801 - val_species_loss: 0.0776\n",
      "Epoch 4/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9110 - has_owner_loss: 0.2311 - loss: 0.4849 - size_accuracy: 0.9098 - size_loss: 0.2386 - species_accuracy: 0.9953 - species_loss: 0.0149 - val_has_owner_accuracy: 0.7616 - val_has_owner_loss: 0.4803 - val_loss: 1.2055 - val_size_accuracy: 0.7550 - val_size_loss: 0.6366 - val_species_accuracy: 0.9735 - val_species_loss: 0.1033\n",
      "Epoch 5/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - has_owner_accuracy: 0.9087 - has_owner_loss: 0.2155 - loss: 0.4490 - size_accuracy: 0.9098 - size_loss: 0.2281 - species_accuracy: 0.9965 - species_loss: 0.0116 - val_has_owner_accuracy: 0.7483 - val_has_owner_loss: 0.4919 - val_loss: 1.2290 - val_size_accuracy: 0.7616 - val_size_loss: 0.6006 - val_species_accuracy: 0.9338 - val_species_loss: 0.1543\n",
      "Epoch 6/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9508 - has_owner_loss: 0.1583 - loss: 0.3320 - size_accuracy: 0.9415 - size_loss: 0.1651 - species_accuracy: 0.9977 - species_loss: 0.0095 - val_has_owner_accuracy: 0.7616 - val_has_owner_loss: 0.5607 - val_loss: 1.2422 - val_size_accuracy: 0.7748 - val_size_loss: 0.5938 - val_species_accuracy: 0.9801 - val_species_loss: 0.0833\n",
      "Epoch 7/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9508 - has_owner_loss: 0.1355 - loss: 0.2802 - size_accuracy: 0.9450 - size_loss: 0.1388 - species_accuracy: 0.9977 - species_loss: 0.0070 - val_has_owner_accuracy: 0.7616 - val_has_owner_loss: 0.4969 - val_loss: 1.2311 - val_size_accuracy: 0.7616 - val_size_loss: 0.6357 - val_species_accuracy: 0.9801 - val_species_loss: 0.0837\n",
      "Epoch 8/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 184ms/step - has_owner_accuracy: 0.9532 - has_owner_loss: 0.1302 - loss: 0.2585 - size_accuracy: 0.9567 - size_loss: 0.1238 - species_accuracy: 0.9977 - species_loss: 0.0059 - val_has_owner_accuracy: 0.7682 - val_has_owner_loss: 0.6991 - val_loss: 1.4896 - val_size_accuracy: 0.7483 - val_size_loss: 0.6913 - val_species_accuracy: 0.9801 - val_species_loss: 0.0929\n",
      "Epoch 9/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9684 - has_owner_loss: 0.0989 - loss: 0.1922 - size_accuracy: 0.9696 - size_loss: 0.0887 - species_accuracy: 1.0000 - species_loss: 0.0027 - val_has_owner_accuracy: 0.7550 - val_has_owner_loss: 0.5549 - val_loss: 1.4052 - val_size_accuracy: 0.7483 - val_size_loss: 0.7549 - val_species_accuracy: 0.9801 - val_species_loss: 0.0866\n",
      "Epoch 10/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 184ms/step - has_owner_accuracy: 0.9731 - has_owner_loss: 0.0865 - loss: 0.1829 - size_accuracy: 0.9684 - size_loss: 0.0923 - species_accuracy: 1.0000 - species_loss: 0.0027 - val_has_owner_accuracy: 0.7616 - val_has_owner_loss: 0.6130 - val_loss: 1.4298 - val_size_accuracy: 0.7550 - val_size_loss: 0.7220 - val_species_accuracy: 0.9735 - val_species_loss: 0.0937\n",
      "Epoch 11/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 184ms/step - has_owner_accuracy: 0.9719 - has_owner_loss: 0.0822 - loss: 0.1879 - size_accuracy: 0.9707 - size_loss: 0.1017 - species_accuracy: 0.9977 - species_loss: 0.0047 - val_has_owner_accuracy: 0.7616 - val_has_owner_loss: 0.6277 - val_loss: 1.4561 - val_size_accuracy: 0.7881 - val_size_loss: 0.7252 - val_species_accuracy: 0.9801 - val_species_loss: 0.0834\n",
      "Epoch 12/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - has_owner_accuracy: 0.9731 - has_owner_loss: 0.0826 - loss: 0.1540 - size_accuracy: 0.9778 - size_loss: 0.0675 - species_accuracy: 1.0000 - species_loss: 0.0023 - val_has_owner_accuracy: 0.7483 - val_has_owner_loss: 0.7372 - val_loss: 1.6137 - val_size_accuracy: 0.7748 - val_size_loss: 0.7244 - val_species_accuracy: 0.9735 - val_species_loss: 0.1090\n",
      "Epoch 13/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9778 - has_owner_loss: 0.0681 - loss: 0.1174 - size_accuracy: 0.9848 - size_loss: 0.0487 - species_accuracy: 1.0000 - species_loss: 0.0014 - val_has_owner_accuracy: 0.7417 - val_has_owner_loss: 0.7077 - val_loss: 1.6879 - val_size_accuracy: 0.7815 - val_size_loss: 0.8366 - val_species_accuracy: 0.9801 - val_species_loss: 0.1123\n",
      "Epoch 14/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - has_owner_accuracy: 0.9789 - has_owner_loss: 0.0671 - loss: 0.1139 - size_accuracy: 0.9883 - size_loss: 0.0451 - species_accuracy: 1.0000 - species_loss: 0.0021 - val_has_owner_accuracy: 0.7483 - val_has_owner_loss: 0.7200 - val_loss: 1.5651 - val_size_accuracy: 0.7550 - val_size_loss: 0.7122 - val_species_accuracy: 0.9801 - val_species_loss: 0.1039\n",
      "Epoch 15/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9801 - has_owner_loss: 0.0614 - loss: 0.1074 - size_accuracy: 0.9883 - size_loss: 0.0403 - species_accuracy: 0.9988 - species_loss: 0.0046 - val_has_owner_accuracy: 0.7682 - val_has_owner_loss: 0.7586 - val_loss: 1.5961 - val_size_accuracy: 0.7815 - val_size_loss: 0.6905 - val_species_accuracy: 0.9801 - val_species_loss: 0.1043\n",
      "Epoch 16/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9906 - has_owner_loss: 0.0417 - loss: 0.0833 - size_accuracy: 0.9930 - size_loss: 0.0389 - species_accuracy: 1.0000 - species_loss: 0.0025 - val_has_owner_accuracy: 0.7483 - val_has_owner_loss: 0.7405 - val_loss: 1.9679 - val_size_accuracy: 0.7550 - val_size_loss: 1.1120 - val_species_accuracy: 0.9801 - val_species_loss: 0.1048\n",
      "Epoch 17/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9930 - has_owner_loss: 0.0389 - loss: 0.0718 - size_accuracy: 0.9906 - size_loss: 0.0303 - species_accuracy: 1.0000 - species_loss: 0.0031 - val_has_owner_accuracy: 0.7616 - val_has_owner_loss: 0.6884 - val_loss: 1.6051 - val_size_accuracy: 0.7748 - val_size_loss: 0.7900 - val_species_accuracy: 0.9669 - val_species_loss: 0.1081\n",
      "Epoch 18/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9871 - has_owner_loss: 0.0409 - loss: 0.0699 - size_accuracy: 0.9930 - size_loss: 0.0282 - species_accuracy: 1.0000 - species_loss: 6.1800e-04 - val_has_owner_accuracy: 0.7351 - val_has_owner_loss: 0.7562 - val_loss: 1.6912 - val_size_accuracy: 0.7815 - val_size_loss: 0.8017 - val_species_accuracy: 0.9735 - val_species_loss: 0.1115\n",
      "Epoch 19/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 184ms/step - has_owner_accuracy: 0.9918 - has_owner_loss: 0.0302 - loss: 0.0615 - size_accuracy: 0.9941 - size_loss: 0.0290 - species_accuracy: 0.9988 - species_loss: 0.0023 - val_has_owner_accuracy: 0.7815 - val_has_owner_loss: 0.7595 - val_loss: 1.8151 - val_size_accuracy: 0.7417 - val_size_loss: 0.8636 - val_species_accuracy: 0.9603 - val_species_loss: 0.1392\n",
      "Epoch 20/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - has_owner_accuracy: 0.9918 - has_owner_loss: 0.0285 - loss: 0.0534 - size_accuracy: 0.9941 - size_loss: 0.0233 - species_accuracy: 1.0000 - species_loss: 0.0013 - val_has_owner_accuracy: 0.7616 - val_has_owner_loss: 0.7274 - val_loss: 1.8075 - val_size_accuracy: 0.7616 - val_size_loss: 0.9654 - val_species_accuracy: 0.9801 - val_species_loss: 0.1055\n",
      "Epoch 21/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - has_owner_accuracy: 0.9941 - has_owner_loss: 0.0220 - loss: 0.0452 - size_accuracy: 0.9941 - size_loss: 0.0213 - species_accuracy: 1.0000 - species_loss: 0.0022 - val_has_owner_accuracy: 0.7682 - val_has_owner_loss: 0.8115 - val_loss: 1.8992 - val_size_accuracy: 0.7417 - val_size_loss: 0.9112 - val_species_accuracy: 0.9669 - val_species_loss: 0.1308\n",
      "Epoch 22/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 186ms/step - has_owner_accuracy: 0.9953 - has_owner_loss: 0.0202 - loss: 0.0554 - size_accuracy: 0.9906 - size_loss: 0.0331 - species_accuracy: 1.0000 - species_loss: 0.0019 - val_has_owner_accuracy: 0.7815 - val_has_owner_loss: 0.7851 - val_loss: 1.9064 - val_size_accuracy: 0.7483 - val_size_loss: 0.9732 - val_species_accuracy: 0.9669 - val_species_loss: 0.1270\n",
      "Epoch 23/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 184ms/step - has_owner_accuracy: 0.9977 - has_owner_loss: 0.0171 - loss: 0.0374 - size_accuracy: 0.9977 - size_loss: 0.0178 - species_accuracy: 1.0000 - species_loss: 0.0020 - val_has_owner_accuracy: 0.7550 - val_has_owner_loss: 0.9010 - val_loss: 2.0810 - val_size_accuracy: 0.7748 - val_size_loss: 1.0498 - val_species_accuracy: 0.9735 - val_species_loss: 0.1286\n",
      "Epoch 24/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 191ms/step - has_owner_accuracy: 0.9918 - has_owner_loss: 0.0218 - loss: 0.0380 - size_accuracy: 0.9977 - size_loss: 0.0154 - species_accuracy: 1.0000 - species_loss: 6.3106e-04 - val_has_owner_accuracy: 0.7682 - val_has_owner_loss: 0.8723 - val_loss: 2.1548 - val_size_accuracy: 0.7550 - val_size_loss: 1.1754 - val_species_accuracy: 0.9735 - val_species_loss: 0.1151\n",
      "Epoch 25/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 192ms/step - has_owner_accuracy: 0.9965 - has_owner_loss: 0.0205 - loss: 0.0337 - size_accuracy: 0.9965 - size_loss: 0.0109 - species_accuracy: 0.9988 - species_loss: 0.0020 - val_has_owner_accuracy: 0.7550 - val_has_owner_loss: 0.9222 - val_loss: 2.0268 - val_size_accuracy: 0.7483 - val_size_loss: 0.9682 - val_species_accuracy: 0.9801 - val_species_loss: 0.1195\n",
      "Epoch 26/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 184ms/step - has_owner_accuracy: 0.9930 - has_owner_loss: 0.0194 - loss: 0.0339 - size_accuracy: 0.9965 - size_loss: 0.0149 - species_accuracy: 1.0000 - species_loss: 6.8502e-04 - val_has_owner_accuracy: 0.7682 - val_has_owner_loss: 0.8999 - val_loss: 2.0310 - val_size_accuracy: 0.7417 - val_size_loss: 0.9837 - val_species_accuracy: 0.9669 - val_species_loss: 0.1271\n",
      "Epoch 27/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 197ms/step - has_owner_accuracy: 0.9918 - has_owner_loss: 0.0214 - loss: 0.0361 - size_accuracy: 0.9965 - size_loss: 0.0139 - species_accuracy: 1.0000 - species_loss: 4.5772e-04 - val_has_owner_accuracy: 0.7748 - val_has_owner_loss: 0.9561 - val_loss: 2.0070 - val_size_accuracy: 0.7550 - val_size_loss: 0.9107 - val_species_accuracy: 0.9801 - val_species_loss: 0.1228\n",
      "Epoch 28/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 199ms/step - has_owner_accuracy: 0.9930 - has_owner_loss: 0.0219 - loss: 0.0370 - size_accuracy: 0.9953 - size_loss: 0.0144 - species_accuracy: 1.0000 - species_loss: 5.5252e-04 - val_has_owner_accuracy: 0.7550 - val_has_owner_loss: 0.9163 - val_loss: 1.9458 - val_size_accuracy: 0.7616 - val_size_loss: 0.9060 - val_species_accuracy: 0.9801 - val_species_loss: 0.1110\n",
      "Epoch 29/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 202ms/step - has_owner_accuracy: 0.9906 - has_owner_loss: 0.0276 - loss: 0.0459 - size_accuracy: 0.9965 - size_loss: 0.0177 - species_accuracy: 1.0000 - species_loss: 2.6094e-04 - val_has_owner_accuracy: 0.7881 - val_has_owner_loss: 0.8285 - val_loss: 2.1199 - val_size_accuracy: 0.7483 - val_size_loss: 1.2274 - val_species_accuracy: 0.9801 - val_species_loss: 0.1056\n",
      "Epoch 30/30\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 199ms/step - has_owner_accuracy: 1.0000 - has_owner_loss: 0.0111 - loss: 0.0294 - size_accuracy: 0.9941 - size_loss: 0.0173 - species_accuracy: 1.0000 - species_loss: 6.6735e-04 - val_has_owner_accuracy: 0.7748 - val_has_owner_loss: 0.8783 - val_loss: 2.0676 - val_size_accuracy: 0.7616 - val_size_loss: 1.0574 - val_species_accuracy: 0.9735 - val_species_loss: 0.1149\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train_dict,\n",
    "    validation_data=(X_val, y_val_dict),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=cb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff6ec61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 346ms/step - has_owner_accuracy: 0.7748 - has_owner_loss: 0.8802 - loss: 2.0676 - size_accuracy: 0.7616 - size_loss: 1.0605 - species_accuracy: 0.9735 - species_loss: 0.1144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0675716400146484,\n",
       " 0.1143902912735939,\n",
       " 0.880168080329895,\n",
       " 1.0605101585388184,\n",
       " 0.7748344540596008,\n",
       " 0.7615894079208374,\n",
       " 0.9735099077224731]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val,y_val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7787a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, img_path):\n",
    "    \"\"\"\n",
    "    Primește modelul și calea către o imagine, returnează predicția pentru acea imagine.\n",
    "    \"\"\"\n",
    "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "    arr = image.img_to_array(img) / 255.0\n",
    "    arr = np.expand_dims(arr, 0)\n",
    "    \n",
    "    species_pred, owner_pred, size_pred = model.predict(arr, verbose=0)\n",
    "    \n",
    "    species_label = 'dog' if np.argmax(species_pred[0]) == 1 else 'cat'\n",
    "    owner_prob = float(owner_pred[0][0])\n",
    "    size_label = ['short', 'medium', 'long'][int(np.argmax(size_pred[0]))]\n",
    "    \n",
    "    # dacă e pisică, ignoră size\n",
    "    if species_label == 'cat':\n",
    "        size_label = None\n",
    "    \n",
    "    return {'species': species_label, 'has_owner_prob': owner_prob, 'size': size_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b56c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicție din CSV: {'species': 'dog', 'has_owner_prob': 0.26523563265800476, 'size': 'medium'}\n"
     ]
    }
   ],
   "source": [
    "example_filename = df['filename'].iloc[7]  # ia prima imagine din CSV\n",
    "result = predict_image(model, IMAGES_DIR / example_filename)\n",
    "print(\"Predicție din CSV:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9444ba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicție din folder: {'species': 'dog', 'has_owner_prob': 0.44893679022789, 'size': 'big'}\n"
     ]
    }
   ],
   "source": [
    "folder = IMAGES_DIR / \"dog\"  # sau \"cat_small\", după caz\n",
    "example_file = next(folder.glob(\"dog10.jpg\"))  # ia primul fișier jpg\n",
    "result2 = predict_image(model, example_file)\n",
    "print(\"Predicție din folder:\", result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda10a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
